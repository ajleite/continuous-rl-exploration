{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code can be used to add a folder in the repository to the Python import\n",
    "# path, irrespective of whether the notebook is being run in colab or Jupyter.\n",
    "# (C) 2020 Abe Leite, Indiana University Bloomington\n",
    "# This code block is released under MIT license. Feel free to make use of\n",
    "# this code in any projects so long as you reproduce this text.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "repo_URL = 'https://github.com/ajleite/continuous-rl-exploration'\n",
    "repo_name = repo_URL.split('/')[-1]\n",
    "# top level of repository. --AL Mar 10 2022\n",
    "code_folder = ''\n",
    "\n",
    "try:\n",
    "  repo_path = subprocess.check_output('git rev-parse --show-toplevel', shell=True).decode().strip()\n",
    "except subprocess.CalledProcessError:\n",
    "  os.system(f'git clone {repo_URL} --depth 1')\n",
    "  repo_path = os.path.abspath(repo_name)\n",
    "\n",
    "code_path = os.path.join(repo_path, code_folder)\n",
    "sys.path.append(code_path)\n",
    "print(f'Loading code from {code_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget the boilerplate --AL Mar 10 2022\n",
    "!apt-get update\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet\n",
    "!pip install gym\n",
    "!pip install pybullet\n",
    "!git clone https://github.com/benelot/pybullet-gym.git\n",
    "cd /content/pybullet-gym/\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110420dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll go to the repo path in order to work with saved models. --AL Mar 10 2022\n",
    "os.chdir(code_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf43efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import pybullet_data\n",
    "import pybulletgym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe12200",
   "metadata": {},
   "source": [
    "# 0 Tasks\n",
    "\n",
    "For this assignment I make use of two tasks: the cart-pole task and the half-cheetah task. I make small modifications to each of them, which I describe below.\n",
    "\n",
    "## 0.1 Cart-pole task\n",
    "\n",
    "The cart-pole task is a classic control task introduced by Sutton and Barto in which a wheeled cart is mounted on a frictionless, finite-length beam. On top of the cart a pole is balanced. The agent's goal is to exert forces on the cart to prevent the pole from falling over without pushing the cart off of the edge of the beam.\n",
    "\n",
    "For this task, I slightly modified the reward schedule to better reflect the structure of the task. This only affects the reward signals provided to the agent and not the episode returns shown in the graphs. The original cart-pole task does not distinguish between the case where the episode terminates naturally after 1000 timesteps and the case where the pole falls at timestep 1000. Additionally, by providing a positive reward for every timestep when the agent survives, it creates a slow-to-learn value function which will become, except in doomed trajectories, the constant-reward scale factor determined by the formula $\\frac{1}{1-\\gamma}$. Finally, the agent will be confused that the cumulative return for a timestep soon before the 1000-timestep cutoff will appear lower than the cumulative return at an earlier timestep even with an optimal policy. I rectify all of these issues by providing the agent with a 0 reward signal for every timestep that the pole does not fall and a -1 reward signal for any timestep that the pole does fall. The code that implements this version of the cart-pole problem is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcacc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleTask:\n",
    "    def __init__(self, rng):\n",
    "        self.cartpole_env = gym.make(\"InvertedPendulumMuJoCoEnv-v0\")\n",
    "        self.cartpole_env.seed(int(rng.integers(2**63-1)))\n",
    "        self.obs_shape = (4,)\n",
    "        self.action_shape = (1,)\n",
    "        self.ts = 0\n",
    "    def reset(self):\n",
    "        self.ts = 0\n",
    "        return self.cartpole_env.reset()\n",
    "    def step(self, action):\n",
    "        o, r, t, _ = self.cartpole_env.step(action)\n",
    "        self.ts += 1\n",
    "        return o, -1. if t and self.ts != 1000 else 0., t, _\n",
    "    def render(self):\n",
    "        return self.cartpole_env.render()\n",
    "    def get_return(self):\n",
    "        return self.ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa72be",
   "metadata": {},
   "source": [
    "## 0.2 Half-cheetah task\n",
    "\n",
    "The half-cheetah task is a locomotion task in which an agent controls a two-dimensional cheetah-like body which is only stable when both legs are on the ground. As such, it can only move far distances by galloping rapidly, resulting in a deceptive-reward problem where the locally optimal behavior is always to take a single step forwards. The half-cheetah's body has 6 joints which are controlled by the agent and 17 proprioceptive position and velocity sensors which provide the agent with input. The agent is rewarded for forwards locomotion and penalized for energy cost.\n",
    "\n",
    "I made only minimal modifications to the half-cheetah task. In order to reduce the dataset's bias towards states where the cheetah has fallen over, I early-terminate all episodes at 500 timesteps. This affects all episode return calculations. In cases where the agent is successfully galloping, this would roughly half the total return that the agent receives; in cases where the agent becomes stuck and does not move, it does not affect the total return. In cases where the agent is continually exerting torques but not moving successfully, it would roughly half the (negative) total return that the agent receives. The code that implements this version of the half-cheetah task is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfCheetahTask:\n",
    "    def __init__(self, rng, to_render=False):\n",
    "        self.env = gym.make(\"HalfCheetahMuJoCoEnv-v0\")\n",
    "        if to_render:\n",
    "            self.env.render('human')\n",
    "        self.env.seed(int(rng.integers(2**63-1)))\n",
    "\n",
    "        self.obs_shape = (17,)\n",
    "        self.action_shape = (6,)\n",
    "        self.cumulative_r = 0\n",
    "        self.timestep = 0\n",
    "    def reset(self):\n",
    "        self.cumulative_r = 0\n",
    "        self.timestep = 0\n",
    "        return self.env.reset()\n",
    "    def step(self, action):\n",
    "        o, r, t, _ = self.env.step(action)\n",
    "        self.timestep += 1\n",
    "        self.cumulative_r += r\n",
    "        return o, r, t or self.timestep == 500, _\n",
    "    def render(self):\n",
    "        return self.env.render('human')\n",
    "    def get_return(self):\n",
    "        return self.cumulative_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edefd84",
   "metadata": {},
   "source": [
    "## 0.3 Simulation model\n",
    "\n",
    "The following cell contains boilerplate code for training an agent on an environment and saving the results. It greedily evaluates the policy every 50 training episodes in order to track how performance changes over time and preserve the best-performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self, agent, task, num_episodes, path=None):\n",
    "        self.agent = agent\n",
    "        self.task = task\n",
    "\n",
    "        self.num_episodes = num_episodes\n",
    "\n",
    "        self.training_episode_rewards = []\n",
    "        self.eval_episode_rewards = []\n",
    "\n",
    "        self.value_rmse = []\n",
    "        self.mean_obj = []\n",
    "\n",
    "        self.best_weights = None\n",
    "        self.best_100_episode_return = None\n",
    "        self.path = path\n",
    "\n",
    "    def save_trace(self):\n",
    "        if not self.path:\n",
    "            return\n",
    "\n",
    "        to_save = {'best_weights': self.best_weights, 'best_100_episode_return': self.best_100_episode_return,\n",
    "            'training_episode_rewards': self.training_episode_rewards, 'eval_episode_rewards': self.eval_episode_rewards,\n",
    "            'value_rmse': self.value_rmse, 'mean_obj': self.mean_obj}\n",
    "\n",
    "        pickle.dump(to_save, open(self.path,'wb'))\n",
    "\n",
    "    def evaluate(self, n, render=False):\n",
    "        eval_rewards = []\n",
    "        eval_lengths = []\n",
    "        for i in range(100):\n",
    "            s = self.task.reset()\n",
    "            t = False\n",
    "\n",
    "            while not t:\n",
    "                a = self.agent.act(s, 0, temp=0)\n",
    "\n",
    "                s2, r, t, _ = self.task.step(a)\n",
    "                s = s2\n",
    "\n",
    "                if render and i == 0:\n",
    "                    self.task.render()\n",
    "\n",
    "            # episode is over, record it\n",
    "            eval_rewards.append(self.task.get_return())\n",
    "            print((n, 'e', i, self.task.get_return()))\n",
    "\n",
    "        self.eval_episode_rewards.append(eval_rewards)\n",
    "\n",
    "        # maintain best stats\n",
    "        if self.best_100_episode_return is None or np.mean(eval_rewards) > self.best_100_episode_return:\n",
    "            self.best_100_episode_return = np.mean(eval_rewards)\n",
    "            self.best_weights = self.agent.get_weights()\n",
    "        print('cycle', n, 'mean eval:', np.mean(eval_rewards), 'best:', self.best_100_episode_return)\n",
    "        self.save_trace()\n",
    "\n",
    "    def run(self, render=False):\n",
    "        timestep = 0\n",
    "\n",
    "        for n in range(self.num_episodes):\n",
    "            # 1. gather training trajectories\n",
    "            training_rewards = []\n",
    "            training_lengths = []\n",
    "\n",
    "            for traj in range(self.agent.actor_count):\n",
    "                s = self.task.reset()\n",
    "                t = False\n",
    "\n",
    "                while not t:\n",
    "                    a = self.agent.act(s, traj)\n",
    "\n",
    "                    s2, r, t, _ = self.task.step(a)\n",
    "                    self.agent.store(traj, s, a, r, t)\n",
    "                    s = s2\n",
    "\n",
    "                    timestep += 1\n",
    "\n",
    "                    if render and traj == 0:\n",
    "                        self.task.render()\n",
    "\n",
    "                # episode is over, record it\n",
    "                training_rewards.append(self.task.get_return())\n",
    "                print((n, 't', traj, self.task.get_return()))\n",
    "\n",
    "            self.training_episode_rewards.append(training_rewards)\n",
    "\n",
    "            # 2. train for 16 epochs\n",
    "            value_rmse, mean_obj = self.agent.train(16)\n",
    "            self.value_rmse.append(value_rmse)\n",
    "            self.mean_obj.append(mean_obj)\n",
    "            print(n, 'l', value_rmse, mean_obj)\n",
    "\n",
    "            # 3. gather greedy evaluation trajectories every 50 training episodes\n",
    "            if n % (50 / self.agent.actor_count) >= 1:\n",
    "                continue\n",
    "\n",
    "            self.evaluate(n, render)\n",
    "\n",
    "        # everything is done!\n",
    "        self.evaluate(self.num_episodes, render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3bd8d",
   "metadata": {},
   "source": [
    "# 1 Network design\n",
    "\n",
    "## 1.1 Probabilistic policy using mixture-of-Gaussians\n",
    "\n",
    "I use a coordinate-wise mixture-of-Gaussians distribution as my stochastic policy; as such, my networks each output the paramaters for this distribution. One regret is that each action coordinate is independent of the others (each of the n action coordinates is determined using a univariate mixture-of-Gaussians, rather than the entire action being determined using an n-dimensional multivariate mixture-of-Gaussians). This prevents the multi-coordinate actions from being as coordinated as they would otherwise be.\n",
    "\n",
    "Concretely, each mode of the multivariate Gaussian distribution has a weight, a mean, and a standard deviation. The shape of the policy network's output is thus `(n_action_coordinates, n_action_modes, 3)`, where the 3-dimension axis contains the weight, the mean, and the standard deviation. The weights are coerced into a discrete probability distribution using the softmax transformation, and the standard deviations are made non-negative using a sigmoid function. The sigmoid function's output is divided by the action's dimension in order to limit the total variance over all action coordinates. The means are clipped to the `[-1, 1]` range but are otherwise a direct linear transformation of the previous layer's output.\n",
    "\n",
    "In order to maintain time-correlation for actions and allow for proper credit assignment, I recalculate the probability distribution coordinates for each mode selection and each Gaussian sample once every handful of frames according to a hyperparameter, `stoch_persistence`.\n",
    "\n",
    "The code to select actions according to this probabilistic policy, calculate the log-probability of actions in the policy, and calculate the entropy of the policy (with certain limitations) is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24196d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageAgent:\n",
    "    def __init__(self, rng, actor_count, policy_network, value_network, t_max, discount_factor, stoch_persistence, entropy_weight):\n",
    "        self.rng = rng\n",
    "\n",
    "        self.actor_count = actor_count\n",
    "\n",
    "        self.policy_network = policy_network\n",
    "        self.value_network = value_network\n",
    "\n",
    "        self.t_max = t_max\n",
    "        self.discount_factor = discount_factor\n",
    "        self.stoch_persistence = stoch_persistence\n",
    "\n",
    "        self.beta_entropy_weight = entropy_weight\n",
    "        self.minibatch_size = 128\n",
    "        self.use_tqdm = False\n",
    "\n",
    "        self.obs_shape = self.policy_network.obs_shape\n",
    "        self.action_shape = self.policy_network.action_shape\n",
    "        self.action_modes = self.policy_network.action_modes\n",
    "\n",
    "        self.stoch_mode_t = self.rng.random((self.actor_count,) + self.action_shape)\n",
    "        self.stoch_gauss_t = self.rng.random((self.actor_count,) + self.action_shape)\n",
    "        # this is a prominent term in the Gaussian quantile formula, precomputed for efficiency\n",
    "        self.stoch_gauss_inverf = np.sqrt(2) * scipy.special.erfinv(2 * self.stoch_gauss_t - 1)\n",
    "        # this is a normalization factor to bias exploration down with high-dimensional actions\n",
    "        self.stdev_norm = np.prod(self.action_shape)\n",
    "\n",
    "        self.experience_buffer = [experience_store.NStepTDBuffer(self.obs_shape, self.action_shape, self.t_max, self.discount_factor) for _ in range(self.actor_count)]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.policy_network.keras_network.get_weights(), self.value_network.keras_network.get_weights()\n",
    "\n",
    "    def act(self, obs, actor=0, temp=1):\n",
    "        if self.rng.random() > self.stoch_persistence:\n",
    "            self.stoch_mode_t[actor] = self.rng.random(self.action_shape)\n",
    "            self.stoch_gauss_t[actor] = self.rng.random(self.action_shape)\n",
    "            self.stoch_gauss_inverf[actor] = np.sqrt(2) * scipy.special.erfinv(2 * self.stoch_gauss_t[actor] - 1)\n",
    "\n",
    "        # shape: self.action_shape + (self.action_modes, 3), where the inner three values are the weight, mean, and standard deviation for each mode\n",
    "        output = np.array(self.policy_network.apply(np.expand_dims(obs, axis=0))[0])\n",
    "        weights = output[..., 0]\n",
    "        pre_means = output[..., 1]\n",
    "        pre_stdevs = output[..., 2]\n",
    "\n",
    "        softmax_weights = np.exp(weights) / np.sum(np.exp(weights), axis=-1, keepdims=True)\n",
    "\n",
    "        means = np.clip(pre_means, -1, 1)\n",
    "\n",
    "        # this is the formula for the sigmoid function, with certain biases added\n",
    "        stdevs = 1/(1 + np.exp(pre_stdevs))/self.stdev_norm+0.0001\n",
    "\n",
    "        cum_weights = np.cumsum(softmax_weights, axis=-1)\n",
    "        sel_mode = np.argmax(np.expand_dims(self.stoch_mode_t[actor], axis=-1) < cum_weights, axis=-1, keepdims=True)\n",
    "\n",
    "        sel_means = np.squeeze(np.take_along_axis(means, sel_mode, axis=-1), axis=-1)\n",
    "        sel_stdevs = np.squeeze(np.take_along_axis(stdevs, sel_mode, axis=-1), axis=-1)\n",
    "\n",
    "        # this is the quantile formula of the Gaussian distribution\n",
    "        sel_actions = sel_means + temp * sel_stdevs * self.stoch_gauss_inverf[actor]\n",
    "\n",
    "        return sel_actions\n",
    "\n",
    "    @tf.function\n",
    "    def log_prob_actions(self, obss, actions):\n",
    "        # take the weighted probability of each coordinate of each action over modes,\n",
    "        #   then transform into log space and add over coordinates\n",
    "        # maintains the batch axis\n",
    "\n",
    "        # shape: (n,) + self.action_shape + (self.action_modes, 3)\n",
    "        outputs = self.policy_network.apply(obss)\n",
    "        weights = outputs[..., 0]\n",
    "        pre_means = outputs[..., 1]\n",
    "        pre_stdevs = outputs[..., 2]\n",
    "\n",
    "        softmax_weights = tf.nn.softmax(weights, axis=-1)\n",
    "\n",
    "        means = tf.clip_by_value(pre_means, -1, 1)\n",
    "\n",
    "        stdevs = tf.nn.sigmoid(pre_stdevs)/self.stdev_norm+0.0001\n",
    "\n",
    "        # want to get the PDF of each real action in each of the modes\n",
    "        # this is the PDF formula of the Gaussian distribution\n",
    "        mode_PDFs = tf.exp(-0.5 * ((means - tf.expand_dims(actions, axis=-1)) / stdevs)**2) / (stdevs * np.sqrt(2*np.pi))\n",
    "\n",
    "        # take the weighted average over modes to get the probability of each coordinate of each action\n",
    "        weighted_PDF = tf.reduce_sum(softmax_weights * mode_PDFs, axis=-1)+0.0001\n",
    "        log_PDF = tf.math.log(weighted_PDF)\n",
    "\n",
    "        # add in log space to get joint probability of all action coordinates, keeping the batch axis 0\n",
    "        total_log_PDF = tf.reduce_sum(log_PDF, axis=range(1, len(log_PDF.shape)))\n",
    "\n",
    "        return total_log_PDF\n",
    "\n",
    "    @tf.function\n",
    "    def action_entropies(self, obss):\n",
    "        # Gets the entropy of the action distribution for each observation.\n",
    "        # Unfortunately, I did not have time to find an analytic solution\n",
    "        #   to the problem of finding the entropy of the mixture-of-Gaussians\n",
    "        #   distribution. This is an open problem as described in\n",
    "        #   https://isas.iar.kit.edu/pdf/MFI08_HuberBailey.pdf.\n",
    "        # I have some novel techniques for describing the information theory\n",
    "        #   of continuous random variables but I don't know if they will apply\n",
    "        #   here.\n",
    "        # Instead, I take the approach of overestimating entropy by assuming\n",
    "        #   zero overlap between the component distributions.\n",
    "\n",
    "        # shape: (n,) + self.action_shape + (self.action_modes, 3)\n",
    "        outputs = self.policy_network.apply(obss)\n",
    "        weights = outputs[..., 0]\n",
    "        pre_means = outputs[..., 1]\n",
    "        pre_stdevs = outputs[..., 2]\n",
    "        stdevs = tf.nn.sigmoid(pre_stdevs)/self.stdev_norm+0.0001\n",
    "\n",
    "        means = tf.clip_by_value(pre_means, -1, 1)\n",
    "\n",
    "        # By my bag-of-tricks theorem (there is probably a better name for it somewhere),\n",
    "        #   the entropy of a discrete mixture of RVs, selected according to some index RV\n",
    "        #   X, is equal to to entropy of X plus the expected entropy of the selected RV.\n",
    "        # Do not worry about the fact that I am mixing differential and discrete entropies.\n",
    "        #   In my work unifying discrete and continuous information theory, I have shown\n",
    "        #   that differential entropy is the finite deviation, in bits or nats, between the infinite\n",
    "        #   entropies of your continuous random variable of interest and the unit uniform\n",
    "        #   random variable of the same dimension.\n",
    "        # So differential entropy and discrete entropy have the same units - nats - and all is well.\n",
    "\n",
    "        mode_entropies = 0.5 * tf.math.log(2 * np.pi * stdevs**2) + 0.5\n",
    "\n",
    "        # Because of clipping at +/-1, the above formula isn't quite right. Penalize means near +/- 1.\n",
    "        mode_entropies -= means**4\n",
    "\n",
    "        if self.action_modes > 1:\n",
    "            softmax_weights = tf.nn.softmax(weights, axis=-1)\n",
    "            weight_entropy_terms = -tf.math.log(softmax_weights)\n",
    "            coordinate_entropies = tf.reduce_sum(tf.math.multiply_no_nan(weight_entropy_terms + mode_entropies, softmax_weights), axis=-1)\n",
    "        else:\n",
    "            coordinate_entropies = tf.reduce_sum(mode_entropies, axis=-1)\n",
    "\n",
    "        # now we add the entropy of each coordinate since they are independent in this version.\n",
    "        total_entropies = tf.reduce_sum(coordinate_entropies, axis=range(1, len(coordinate_entropies.shape)))\n",
    "\n",
    "        return total_entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78b080",
   "metadata": {},
   "source": [
    "## 1.2 Network design (general)\n",
    "\n",
    "In general, the networks' designs are quite simple; each consists of a number of feed-forward ReLU layers followed by an output layer whose shape is `(1,)` if the network is a value network or `(n_action_coordinates, n_action_modes, 3)` if the network is a policy network. The code necessary to support this is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFANN_factory(hidden_layer_sizes):\n",
    "    def network_factory(obs_input):\n",
    "        next_input = obs_input\n",
    "        for hidden_layer, hidden_layer_size in enumerate(hidden_layer_sizes):\n",
    "            next_input = tf.keras.layers.Dense(hidden_layer_size, activation='relu', kernel_initializer='random_normal', bias_initializer='random_normal')(next_input)\n",
    "        return next_input\n",
    "\n",
    "    return network_factory\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, obs_shape, network_factory, learning_rate, is_policy, action_shape=None, action_modes=None):\n",
    "        self.obs_shape = obs_shape\n",
    "        self.network_factory = network_factory\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_policy = is_policy\n",
    "        self.action_shape = action_shape\n",
    "        self.action_modes = action_modes\n",
    "\n",
    "        obs_input = tf.keras.layers.Input(shape=obs_shape)\n",
    "\n",
    "        last_layer = network_factory(obs_input)\n",
    "\n",
    "        if is_policy:\n",
    "            output_shape = action_shape + (action_modes, 3)\n",
    "            output_size = np.prod(output_shape)\n",
    "        else:\n",
    "            output_shape = (1,)\n",
    "            output_size = 1\n",
    "\n",
    "        flat_linear_output = tf.keras.layers.Dense(output_size)(last_layer)\n",
    "        linear_output = tf.keras.layers.Reshape(output_shape)(flat_linear_output)\n",
    "\n",
    "        self.keras_network = tf.keras.Model(obs_input, linear_output)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def apply(self, S):\n",
    "        return self.keras_network(S)\n",
    "\n",
    "    def copy_from(self, other, amount):\n",
    "        for self_w, other_w in zip(self.keras_network.weights, other.keras_network.weights):\n",
    "            self_w.assign(self_w*(1-amount) + other_w*amount)\n",
    "\n",
    "    def copy(self):\n",
    "        other = Network(self.obs_shape, self.network_factory, self.learning_rate, self.is_policy, self.action_shape, self.action_modes)\n",
    "        other.copy_from(self, 1)\n",
    "        return other\n",
    "\n",
    "    def zero_like(self):\n",
    "        other = Network(self.obs_shape, self.network_factory, self.learning_rate, self.is_policy, self.action_shape, self.action_modes)\n",
    "        for other_w in other.keras_network.weights:\n",
    "            other_w.assign(tf.zeros_like(other_w))\n",
    "        return other\n",
    "\n",
    "    def zero_self(self):\n",
    "        for w in self.keras_network.weights:\n",
    "            w.assign(tf.zeros_like(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c913e3d",
   "metadata": {},
   "source": [
    "## 1.3 Network design (cart-pole)\n",
    "\n",
    "The cart-pole network design includes a 4-unit input layer from the environment, followed by 40- and 20-unit hidden ReLU layers, and finally either a 1-unit (value networks), 3-unit (policy network for REINFORCE), or 9-unit (policy network for A2C) linear output. The reason for the difference between REINFORCE's policy network and A2C's is that I used a unimodal policy distribution for REINFORCE, thinking it to be closer to the approach taken in the original paper. I use fully separate value and policy networks for program simplicity. These networks were optimized using Adam with a training rate of 0.0001. Although they are duplicated in Section 2, I have included the network specifications for both cart-pole experiments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ac250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE\n",
    "# policy_network = network.Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, True, task.action_shape, 1)\n",
    "# value_network = network.Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, False, task.action_shape, 1)\n",
    "\n",
    "# A2C\n",
    "# policy_network = network.Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, True, task.action_shape, 3)\n",
    "# value_network = network.Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, False, task.action_shape, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e4e43",
   "metadata": {},
   "source": [
    "## 1.4 Network design (half-cheetah)\n",
    "\n",
    "The half-cheetah network design includes a 17-unit input layer from the environment, followed by 100- and 50-unit hidden ReLU layers, and finally either a 1-unit (value networks) or 18-unit (policy network) linear output. I am using larger hidden layers here to represent the numerous disentangled variables the network must maintain to successfully perform this task. Again, I use fully separate value and policy networks for program simplicity. These networks were optimized using Adam with a training rate of 0.00005. Although they are duplicated in Section 2, I have included the network specifications for the half-cheetah experiments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.0001, True, task.action_shape, 1)\n",
    "# value_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.0001, False, task.action_shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654972e",
   "metadata": {},
   "source": [
    "# 2 Training algorithms\n",
    "\n",
    "## 2.1 Common features\n",
    "\n",
    "REINFORCE and A2C are closely related reinforcement learning algorithms for probabilistic policies on continuous-action problems. Each of them is predicated on the idea of reinforcing actions that yield better-than-average results for the current policy. As such, each algorithm is highly dependent on on-policy data: without knowing the true average returns for each state it is impossible to know whether a particular action had positive advantage or not. The function representing the on-policy expected return of a state is called a baseline in REINFORCE and a value function in A2C.\n",
    "\n",
    "In REINFORCE, the baseline is optional, though it is for practical purposes necessary whenever the mean return deviates from zero. In A2C, the value function is mandatory. In REINFORCE, the baseline is always trained on full rollout data, whereas in A2C it is often trained in the `n`-step TD manner where `n` is significantly less than the mean trajectory length.\n",
    "\n",
    "In addition to network-level hyperparameters like training rate and policy-level hyperparameters like action persistence, one hyperparameter common to REINFORCE and A2C is the temporal discount factor, which is used in both the Monte-Carlo and the `n`-step TD calculations.\n",
    "\n",
    "Code for training an `AdvantageAgent` that supports the key features of both REINFORCE and A2C is included below. Note that the agent's methods to act, calculate the log-probability of an action, and calculate the entropy of an action were all defined in Section 1.1. The code for the agent's experience store (which is cleared after every training cycle) is also included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152656fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepTDBuffer:\n",
    "    def __init__(self, obs_shape, action_shape, t_max, discount_factor, buffer_size=10000):\n",
    "        self.S_samples = np.zeros((buffer_size,)+obs_shape, dtype=np.float32)\n",
    "        self.A_samples = np.zeros((buffer_size,)+action_shape, dtype=np.float32)\n",
    "        self.R_samples = np.zeros((buffer_size,), dtype=np.float32)\n",
    "        self.dt_samples = np.zeros((buffer_size,), dtype=np.int32)\n",
    "        self.S2_samples = np.zeros((buffer_size,)+obs_shape, dtype=np.float32)\n",
    "\n",
    "        self.t_max = t_max\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # buffer_size represents the size of the buffer.\n",
    "        # cur_index represents the next index that will be written.\n",
    "        # filled represents whether the buffer has been filled at least once (can be sampled freely).\n",
    "        self.buffer_size = buffer_size\n",
    "        self.cur_index = 0\n",
    "        self.filled = False\n",
    "\n",
    "        self.episode_buffer = []\n",
    "\n",
    "    def store_episode(self, observations, actions, rewards):\n",
    "        ''' store_episode should be called at the end of an episode.\n",
    "\n",
    "            observations, actions, and rewards should be numpy arrays\n",
    "            whose shapes align along their first axis (timestep axis).\n",
    "\n",
    "            - observations should be a float32 array whose subsequent axes match obs_shape.\n",
    "            - actions should be a float32 array whose subsequent axes match action_shape.\n",
    "            - rewards should be a float32 array with no subsequent axis,\n",
    "              and it should represent the rewards for the transitions\n",
    "              following the (observation, action) pairs in the corresponding indices. '''\n",
    "\n",
    "        trajectory_length = rewards.shape[0]\n",
    "\n",
    "        # discard unusable information ASAP\n",
    "        if trajectory_length > self.buffer_size:\n",
    "            print(f\"WARNING: experience buffer overflow (trajectory length {trajectory_length}, buffer size {self.buffer_size}). Clipping beginning of trajectory.\")\n",
    "            observations = observations[-self.buffer_size:]\n",
    "            actions = actions[-self.buffer_size:]\n",
    "            rewards = rewards[-self.buffer_size:]\n",
    "            trajectory_length = self.buffer_size\n",
    "\n",
    "        t_max = self.t_max\n",
    "        if t_max <= 0:\n",
    "            t_max = trajectory_length\n",
    "\n",
    "        # calculate trajectory rewards for each timestep\n",
    "        # (ensure double precision for this calculation)\n",
    "        traj_rewards = np.array(rewards, dtype=np.float64)\n",
    "        for offset in range(1, min(trajectory_length, t_max)):\n",
    "            traj_rewards[:-offset] += rewards[offset:] * self.discount_factor**offset\n",
    "        traj_rewards = np.float32(traj_rewards)\n",
    "        # print(traj_rewards)\n",
    "\n",
    "        if trajectory_length <= t_max:\n",
    "            dt = np.zeros(trajectory_length, dtype=np.int32)\n",
    "            s2 = observations\n",
    "        else:\n",
    "            dt_incomplete = np.full(trajectory_length - t_max, t_max, dtype=np.int32)\n",
    "            dt_complete = np.zeros(t_max, dtype=np.int32)\n",
    "            dt = np.concatenate([dt_incomplete, dt_complete], axis=0)\n",
    "\n",
    "            s2_incomplete = observations[:trajectory_length - t_max]\n",
    "            s2_complete = np.zeros_like(observations[trajectory_length - t_max:])\n",
    "            s2 = np.concatenate([s2_incomplete, s2_complete], axis=0)\n",
    "\n",
    "        # store the trajectory in the buffer\n",
    "        # (split based on whether we will wrap around the end of the buffer)\n",
    "        will_loop = self.cur_index + trajectory_length >= self.buffer_size\n",
    "        if will_loop:\n",
    "            can_store = self.buffer_size - self.cur_index\n",
    "            self.S_samples[self.cur_index:] = observations[:can_store]\n",
    "            self.A_samples[self.cur_index:] = actions[:can_store]\n",
    "            self.R_samples[self.cur_index:] = traj_rewards[:can_store]\n",
    "            self.dt_samples[self.cur_index:] = dt[:can_store]\n",
    "            self.S2_samples[self.cur_index:] = s2[:can_store]\n",
    "\n",
    "            leftover = trajectory_length - can_store\n",
    "            if leftover:\n",
    "                self.S_samples[:leftover] = observations[can_store:]\n",
    "                self.A_samples[:leftover] = actions[can_store:]\n",
    "                self.R_samples[:leftover] = traj_rewards[can_store:]\n",
    "                self.dt_samples[:leftover] = dt[can_store:]\n",
    "                self.S2_samples[:leftover] = s2[can_store:]\n",
    "\n",
    "            self.filled = True\n",
    "            self.cur_index = (self.cur_index + trajectory_length) - self.buffer_size\n",
    "        else:\n",
    "            new_index = self.cur_index + trajectory_length\n",
    "\n",
    "            self.S_samples[self.cur_index:new_index] = observations\n",
    "            self.A_samples[self.cur_index:new_index] = actions\n",
    "            self.R_samples[self.cur_index:new_index] = traj_rewards\n",
    "            self.dt_samples[self.cur_index:new_index] = dt\n",
    "            self.S2_samples[self.cur_index:new_index] = s2\n",
    "\n",
    "            self.cur_index = new_index\n",
    "\n",
    "    def store(self, obs, action, reward, terminal):\n",
    "        ''' This is a convenience function to allow the MonteCarloBuffer to act more like the TD0Buffer.\n",
    "            It matches the signature of TD0Bufer.store. '''\n",
    "\n",
    "        self.episode_buffer.append((obs, action, reward))\n",
    "\n",
    "        # This is an unusual idiom that allows one to effectively take the transpose of a Python list.\n",
    "        # I often use it in RL contexts.\n",
    "        if terminal:\n",
    "            all_S, all_A, all_R = [np.array(all_samples) for all_samples in zip(*self.episode_buffer)]\n",
    "            self.store_episode(all_S, all_A, all_R)\n",
    "            self.episode_buffer = []\n",
    "\n",
    "    def report(self):\n",
    "        if self.filled:\n",
    "            return (self.S_samples, self.A_samples, self.R_samples, self.dt_samples, self.S2_samples)\n",
    "        else:\n",
    "            return (self.S_samples[:self.cur_index], self.A_samples[:self.cur_index], self.R_samples[:self.cur_index],\n",
    "                self.dt_samples[:self.cur_index], self.S2_samples[:self.cur_index])\n",
    "\n",
    "    def clear(self):\n",
    "        self.cur_index = 0\n",
    "        self.filled = False\n",
    "\n",
    "        self.episode_buffer = []\n",
    "\n",
    "@tf.function\n",
    "def AdvantageAgent_train_iter(self, S, A, R):\n",
    "    cur_value = self.value_network.apply(S)[:,0]\n",
    "    adv = R - cur_value\n",
    "\n",
    "    log_prob_actions = self.log_prob_actions(S, A)\n",
    "    action_entropies = self.action_entropies(S)\n",
    "    obj = tf.reduce_sum(log_prob_actions * adv + self.beta_entropy_weight * action_entropies)\n",
    "\n",
    "    obj_gradient = tf.gradients(-obj, self.policy_network.keras_network.weights)\n",
    "    self.policy_network.optimizer.apply_gradients(zip(obj_gradient, self.policy_network.keras_network.weights))\n",
    "\n",
    "    value_loss = tf.reduce_sum(adv ** 2)\n",
    "    value_gradient = tf.gradients(value_loss, self.value_network.keras_network.weights)\n",
    "    self.value_network.optimizer.apply_gradients(zip(value_gradient, self.value_network.keras_network.weights))\n",
    "\n",
    "    return obj, value_loss\n",
    "\n",
    "AdvantageAgent.train_iter = AdvantageAgent_train_iter\n",
    "\n",
    "def AdvantageAgent_train(self, epochs=1):\n",
    "    all_S = []\n",
    "    all_A = []\n",
    "    all_R = []\n",
    "    # S2 and dt are used for the TD updates.\n",
    "    # S2 is the state after all short-term reward has been received.\n",
    "    # dt is the number of timesteps separating S from S2.\n",
    "    # If dt is 0, then S2 is full of garbage values because the episode ended.\n",
    "    all_dt = []\n",
    "    all_S2 = []\n",
    "\n",
    "    for buffer in self.experience_buffer:\n",
    "        S, A, R, dt, S2 = buffer.report()\n",
    "        buffer.clear()\n",
    "\n",
    "        all_S.append(S)\n",
    "        all_A.append(A)\n",
    "        all_R.append(R)\n",
    "        all_dt.append(dt)\n",
    "        all_S2.append(S2)\n",
    "\n",
    "    all_S = np.concatenate(all_S, axis=0)\n",
    "    all_A = np.concatenate(all_A, axis=0)\n",
    "    all_R = np.concatenate(all_R, axis=0)\n",
    "    all_dt = np.concatenate(all_dt, axis=0)\n",
    "    all_S2 = np.concatenate(all_S2, axis=0)\n",
    "\n",
    "    terminal = (all_dt == 0)\n",
    "\n",
    "    # use TD to estimate the total (short-term + long-term) expected reward\n",
    "    if not np.all(terminal):\n",
    "        all_R = np.where(terminal, all_R, all_R + self.discount_factor ** all_dt * np.array(self.value_network.apply(all_S2))[:,0])\n",
    "        all_R = np.float32(all_R)\n",
    "    del all_dt\n",
    "    del all_S2\n",
    "\n",
    "    total_training_samples = all_S.shape[0]\n",
    "    num_minibatches = (total_training_samples * epochs) // self.minibatch_size\n",
    "    last_minibatch_size = (total_training_samples * epochs) % self.minibatch_size\n",
    "\n",
    "    total_obj = 0\n",
    "    total_value_loss = 0\n",
    "\n",
    "    if self.use_tqdm:\n",
    "        import tqdm\n",
    "        minibatches = tqdm.tqdm(range(num_minibatches))\n",
    "    else:\n",
    "        minibatches = range(num_minibatches)\n",
    "\n",
    "    for _ in minibatches:\n",
    "        sample_indices = self.rng.integers(total_training_samples, size=(self.minibatch_size))\n",
    "        S = all_S[sample_indices]\n",
    "        A = all_A[sample_indices]\n",
    "        R = all_R[sample_indices]\n",
    "        if S.size > 0:\n",
    "            obj, value_loss = self.train_iter(S, A, R)\n",
    "            total_obj += obj\n",
    "            total_value_loss += value_loss\n",
    "\n",
    "    if last_minibatch_size:\n",
    "        sample_indices = self.rng.integers(total_training_samples, size=(last_minibatch_size))\n",
    "        S = all_S[sample_indices]\n",
    "        A = all_A[sample_indices]\n",
    "        R = all_R[sample_indices]\n",
    "        if S.size > 0:\n",
    "            obj, value_loss = self.train_iter(S, A, R)\n",
    "            total_obj += obj\n",
    "            total_value_loss += value_loss\n",
    "\n",
    "    value_rmse = np.sqrt(np.array(total_value_loss) / total_training_samples / epochs)\n",
    "    mean_obj = np.array(total_obj) / total_training_samples / epochs\n",
    "\n",
    "    return value_rmse, mean_obj\n",
    "\n",
    "AdvantageAgent.train = AdvantageAgent_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67334590",
   "metadata": {},
   "source": [
    "## 2.2 Unique features of REINFORCE\n",
    "\n",
    "In my implementation, there are no unique features of REINFORCE compared with A2C. It is rather defined by the features it lacks, which will be discussed in the next section.\n",
    "\n",
    "### 2.2.1 Training a policy for the cart-pole task using REINFORCE\n",
    "\n",
    "In order to train my cart-pole policy using REINFORCE, I used a temporal discount factor of 0.95 (reward half-life: 13.5 timesteps, constant reward scaling factor: 20) and an action persistence of 0.965 (expected switch time: 20 timesteps). I trained for 4000 episodes (keeping in mind that episode length is variable based on current performance) and (as mentioned before) the network architectures were [4, 40, 20, 1] (value) and [4, 40, 20, 3] (policy) with a training rate of 0.0001. The full specification for the REINFORCE cart-pole simulation is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc525c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_REINFORCE_cartpole(seed):\n",
    "    agent_rng = np.random.default_rng(seed)\n",
    "    task_rng = np.random.default_rng(seed+234579672983459873)\n",
    "\n",
    "    task = CartPoleTask(task_rng)\n",
    "\n",
    "    path = f'out/REINFORCE-cartpole-{seed}.pickle'\n",
    "\n",
    "    # expected time to switch action distribution is 20 timesteps\n",
    "    policy_network = Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, True, task.action_shape, 1)\n",
    "    value_network = Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, False, task.action_shape, 1)\n",
    "    ag = AdvantageAgent(agent_rng, 1, policy_network, value_network, 0, 0.95, 0.965, 0)\n",
    "\n",
    "    sim = Simulation(ag, task, 4000, path)\n",
    "    sim.run(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ac050",
   "metadata": {},
   "source": [
    "### 2.2.2 Training a policy for the half-cheetah task using REINFORCE\n",
    "\n",
    "In order to train my half-cheetah policy using REINFORCE, I used a temporal discount factor of 0.99 (reward half-life: 62.5 timesteps, constant reward scaling factor: 100) and an action persistence of 0.931 (expected switch time: 10 timesteps). I trained for 2500 episodes (capped at 500 timesteps each) and (as mentioned before) the network architectures were [17, 160, 80, 1] (value) and [17, 160, 180, 18] (policy) with a training rate of 0.00001. The full specification for the REINFORCE half-cheetah simulation is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e20ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_REINFORCE_cheetah(seed):\n",
    "    agent_rng = np.random.default_rng(seed)\n",
    "    task_rng = np.random.default_rng(seed+234579672983459873)\n",
    "\n",
    "    task = tasks.HalfCheetahTask(task_rng)\n",
    "\n",
    "    path = f'out/REINFORCE-cheetah-{seed}.pickle'\n",
    "\n",
    "    # expected time to switch action distribution is 10 timesteps\n",
    "    policy_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, True, task.action_shape, 3)\n",
    "    value_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, False, task.action_shape, 3)\n",
    "    ag = agent.AdvantageAgent(agent_rng, 1, policy_network, value_network, 0, 0.99, 0.931, 0)\n",
    "\n",
    "    sim = simulation.Simulation(ag, task, 2500, path)\n",
    "    sim.run(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e37d60",
   "metadata": {},
   "source": [
    "## 2.3 Unique features of A2C\n",
    "\n",
    "There are three key features that sets A2C apart from REINFORCE. First and foremost, it uses multiple actors to gather multiple trajectories using the same policy. Each actor will at the very least have its own action stochasticity, and in the original A2C implementation, different actors even had different exploration policies (which complicates the theoretical convergence properties of A2C by giving it a heterogeneous policy). This is special because, while each actor's trajectories will be internally correlated along both the actor's action stochasticity and the task's dynamics, the actors' trajectories will not be correlated with each other. This allows the correlated-samples problem to be solved without amassing gigantic datasets and slowing down the rate of policy iteration. (Note that my implementation does not take full advantage of this property since I only perform updates at the end of episodes.)\n",
    "\n",
    "Second, and relatedly, A2C is able to use `n`-step TD bootstrapping while learning state values. While I have fully implemented this, I use Monte-Carlo value estimation to focus more closely on the other differences between my two learning algorithms.\n",
    "\n",
    "Third, A2C avoids premature convergence by including the current policy's entropy over recently-seen states as a term in its objective function.\n",
    "\n",
    "Each of these features was included in the above code for `AdvantageAgent`.\n",
    "\n",
    "### 2.3.1 Training a policy for the cart-pole task using A2C\n",
    "\n",
    "In order to train my cart-pole policy using A2C, I used a temporal discount factor of 0.95 (reward half-life: 13.5 timesteps, constant reward scaling factor: 20) and an action persistence of 0.965 (expected switch time: 20 timesteps). I included an entropy term weighted by 0.05 in my objective function. I used 25 actors and I trained for 160 episode batches (4000 episodes total). As mentioned before, the network architectures were \\[4, 40, 20, 1\\] (value) and \\[4, 40, 20, 9\\] (policy) with a training rate of 0.0001. The full specification for the A2C cart-pole simulation is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_A2C_cartpole(seed):\n",
    "    agent_rng = np.random.default_rng(seed)\n",
    "    task_rng = np.random.default_rng(seed+234579672983459873)\n",
    "\n",
    "    task = tasks.CartPoleTask(task_rng)\n",
    "\n",
    "    path = f'out/A2C-cartpole-{seed}.pickle'\n",
    "\n",
    "    # expected time to switch action distribution is 20 timesteps\n",
    "    policy_network = network.Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, True, task.action_shape, 3)\n",
    "    value_network = network.Network(task.obs_shape, network.FFANN_factory([40, 20]), 0.0001, False, task.action_shape, 3)\n",
    "    ag = agent.AdvantageAgent(agent_rng, 25, policy_network, value_network, 0, 0.95, 0.965, 0.05)\n",
    "\n",
    "    sim = simulation.Simulation(ag, task, 160, path)\n",
    "    sim.run(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23760742",
   "metadata": {},
   "source": [
    "### 2.3.2 Training a policy for the half-cheetah task using A2C\n",
    "\n",
    "In order to train my half-cheetah policy using A2C, I used a temporal discount factor of 0.99 (reward half-life: 62.5 timesteps, constant reward scaling factor: 100) and an action persistence of 0.931 (expected switch time: 10 timesteps). I included an entropy term weighted by 0.05 in my objective function. I used 10 actors and trained for 250 episode cycles (2500 episodes total). As mentioned before, the network architectures were \\[17, 160, 80, 1\\] (value) and \\[17, 160, 180, 18\\] (policy) with a training rate of 0.00001. The full specification for the A2C half-cheetah simulation is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc190f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_A2C_cheetah(seed):\n",
    "    agent_rng = np.random.default_rng(seed)\n",
    "    task_rng = np.random.default_rng(seed+234579672983459873)\n",
    "\n",
    "    task = tasks.HalfCheetahTask(task_rng)\n",
    "\n",
    "    path = f'out/A2C-cheetah-{seed}.pickle'\n",
    "\n",
    "    # expected time to switch action distribution is 20 timesteps\n",
    "    policy_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, True, task.action_shape, 3)\n",
    "    value_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, False, task.action_shape, 3)\n",
    "    ag = agent.AdvantageAgent(agent_rng, 10, policy_network, value_network, 0, 0.99, 0.931, 0.05)\n",
    "\n",
    "    sim = simulation.Simulation(ag, task, 250, path)\n",
    "    sim.run(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae239b9",
   "metadata": {},
   "source": [
    "## 2.4 Main simulations\n",
    "\n",
    "Uncomment the line disabling the following cell and run it to reproduce the simulations for the main assignment. The results are all already saved in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cb0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_simulations_main = False\n",
    "\n",
    "# enable_simulations_main = True\n",
    "\n",
    "if enable_simulations_main:\n",
    "    for i in range(20):\n",
    "        test_REINFORCE_cartpole(i)\n",
    "        test_A2C_cartpole(i)\n",
    "\n",
    "    for i in range(20):\n",
    "        test_REINFORCE_cheetah(i)\n",
    "        test_A2C_cheetah(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef31618",
   "metadata": {},
   "source": [
    "# 3 Experiment results\n",
    "\n",
    "## 3.0 Plotting code\n",
    "\n",
    "The following code was used to report and plot my experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_n_average(values, n):\n",
    "    values = np.array(values)\n",
    "    n = min(n, values.size)\n",
    "    num = np.cumsum(values)\n",
    "    den = np.cumsum(np.ones_like(values))\n",
    "    prev_num = np.concatenate([np.zeros(n), num[:-n]], axis=0)\n",
    "    prev_den = np.concatenate([np.zeros(n), den[:-n]], axis=0)\n",
    "    return (num-prev_num)/(den-prev_den)\n",
    "\n",
    "def load_records(condition, seeds):\n",
    "    records = []\n",
    "    for s in seeds:\n",
    "        records.append(pickle.load(open(f'out/{condition}-{s}.pickle', 'rb')))\n",
    "    return records\n",
    "\n",
    "def plot_training_curves(records, suptitle='Training progress', out_fn=None):\n",
    "    training_returns = [[np.mean(r) for r in record['training_episode_rewards']] for record in records]\n",
    "    evaluation_returns = [[np.mean(r) for r in record['eval_episode_rewards']] for record in records]\n",
    "    value_rmse = [record['value_rmse'] for record in records]\n",
    "    mean_obj = [record['mean_obj'] for record in records]\n",
    "    training_cycle_length = len(records[0]['training_episode_rewards'][0])\n",
    "    training_cycle_count = len(records[0]['training_episode_rewards'])\n",
    "    training_episode_count = training_cycle_length * training_cycle_count\n",
    "\n",
    "    training_episode_numbers = np.arange(0, training_episode_count, training_cycle_length) + training_cycle_length / 2\n",
    "\n",
    "    eval_cycle_length = 50\n",
    "\n",
    "    eval_episode_numbers = np.concatenate([np.arange(0, training_episode_count, eval_cycle_length), [training_episode_count]], axis=0)\n",
    "\n",
    "    plt.figure()\n",
    "    for i, title, x_axis, runs in zip([1,2,3,4], ['On-policy return', 'Greedy return', 'Value loss (RMSE)', 'Mean objective'], [training_episode_numbers, eval_episode_numbers, training_episode_numbers, training_episode_numbers], [training_returns, evaluation_returns, value_rmse, mean_obj]):\n",
    "        if not (runs is training_returns and training_cycle_length != 1 or runs is evaluation_returns):\n",
    "            runs = [last_n_average(run, 100) for run in runs]\n",
    "            # title = title+' (running mean)'\n",
    "\n",
    "        plt.subplot(2,2,i)\n",
    "        plt.title(title)\n",
    "\n",
    "        if len(runs) > 1:\n",
    "            mean_run = np.mean(runs, axis=0)\n",
    "\n",
    "            run_l, run_h = st.t.interval(0.95, len(runs)-1, loc=mean_run, scale=st.sem(runs, axis=0))\n",
    "\n",
    "            plt.fill_between(x_axis, run_l, run_h, color='black', alpha=0.25)\n",
    "            plt.plot(x_axis, run_l, color='black', lw=0.5, ls='--')\n",
    "            plt.plot(x_axis, run_h, color='black', lw=0.5, ls='--', label='95% c.i.')\n",
    "\n",
    "        label = (i == 2)\n",
    "        for run in runs:\n",
    "            if len(runs) == 1:\n",
    "                plt.plot(x_axis, run, label='indiv. run')\n",
    "            elif label:\n",
    "                plt.plot(x_axis, run, alpha=0.25, label='indiv. run')\n",
    "            else:\n",
    "                plt.plot(x_axis, run, alpha=0.25)\n",
    "            label = False\n",
    "\n",
    "        if len(runs) > 1:\n",
    "            plt.plot(x_axis, mean_run, color='red', label='mean run')\n",
    "\n",
    "        if i == 2:\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "\n",
    "        if i > 2:\n",
    "            plt.xlabel('Episode')\n",
    "\n",
    "    plt.suptitle(suptitle)\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.78, hspace=0.3, wspace=0.25)\n",
    "\n",
    "    if not out_fn is None:\n",
    "        plt.savefig(out_fn)\n",
    "        plt.close()\n",
    "\n",
    "def plot_performance(REINFORCE_records, A2C_records, task_name, out_fn=None):\n",
    "    REINFORCE_perfs = [record['best_100_episode_return'] for record in REINFORCE_records]\n",
    "    A2C_perfs = [record['best_100_episode_return'] for record in A2C_records]\n",
    "\n",
    "    print(f\"REINFORCE on {task_name}: {np.mean(REINFORCE_perfs)} +/- {np.std(REINFORCE_perfs)}\")\n",
    "    print(f\"A2C on {task_name}: {np.mean(A2C_perfs)} +/- {np.std(A2C_perfs)}\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(f'Per-run best performance on {task_name} task')\n",
    "    plt.violinplot([REINFORCE_perfs, A2C_perfs], showmeans=True)\n",
    "    plt.gca().xaxis.set_ticks([1,2],['REINFORCE','A2C'])\n",
    "    plt.ylabel('100-episode mean return')\n",
    "\n",
    "    if not out_fn is None:\n",
    "        plt.savefig(out_fn)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abfe9f",
   "metadata": {},
   "source": [
    "## 3.1.1 Results for REINFORCE on the cart-pole task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(load_records(\"REINFORCE-cartpole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec311ee",
   "metadata": {},
   "source": [
    "# 5 Learning the half-cheetah from images\n",
    "\n",
    "## 5.1 Network design\n",
    "\n",
    "Based on my experience with the Pong assignment, I knew that I would not be able to train my full network end-to-end on this problem in a reasonable amount of time. Instead, I turned to a modular approach using a variational autoencoder as a vision module. For this bonus, I used my preferred variational autoencoder implementation, which I wrote two years ago and adapted for this task. This implementation is based on the VAE used in Schmidhuber and Ha's World Models paper. I use the latent representation of the visual stimuli from the half-cheetah task as the input to my agent.\n",
    "\n",
    "The main code for my variational autoencoder is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093cbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closely based on my preferred VAE implementation for my own research, which is in turn\n",
    "# loosely based on https://www.tensorflow.org/tutorials/generative/cvae\n",
    "\n",
    "# loss functions and encoder/decoder architecture borrowed from Ha's repository at\n",
    "# https://github.com/hardmaru/WorldModelsExperiments/blob/master/carracing/vae/vae.py\n",
    "# in order to reproduce the Schmidhuber and Ha results.\n",
    "\n",
    "def log_normal_pdf(x, mean, std_dev):\n",
    "    var = std_dev*std_dev\n",
    "    return -.5 * ((x - mean) ** 2. / var + tf.math.log(var*2*np.pi))\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, inference_conv_layers, generator_deconv_layers, image_shape, latent_dim):\n",
    "        self.image_shape = image_shape\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.inference_layers = [tf.keras.layers.InputLayer(input_shape=image_shape)] + \\\n",
    "            inference_conv_layers + [tf.keras.layers.Flatten(), tf.keras.layers.Dense(2*latent_dim)]\n",
    "        self.inference_net = tf.keras.Sequential(self.inference_layers)\n",
    "        self.generator_layers = [tf.keras.layers.InputLayer(input_shape=(latent_dim,))] + generator_deconv_layers\n",
    "        self.generator_net = tf.keras.Sequential(self.generator_layers)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "\n",
    "    @tf.function\n",
    "    def get_mean(self, sample):\n",
    "        if sample.dtype == tf.uint8:\n",
    "            sample = tf.cast(sample, 'float32')/255\n",
    "        latent_info = self.inference_net(sample)\n",
    "        mean, log_var = tf.split(latent_info, [self.latent_dim, self.latent_dim], -1)\n",
    "        return mean\n",
    "\n",
    "    @tf.function\n",
    "    def sample_latent(self, sample):\n",
    "        if sample.dtype == tf.uint8:\n",
    "            sample = tf.cast(sample, 'float32')/255\n",
    "        if len(sample.shape) == 3:\n",
    "            unbatched = True\n",
    "            sample = tf.expand_dims(sample, axis=0)\n",
    "        else:\n",
    "            unbatched = False\n",
    "\n",
    "        latent_info = self.inference_net(sample)\n",
    "        mean, log_var = tf.split(latent_info, [self.latent_dim, self.latent_dim], -1)\n",
    "\n",
    "        std_dev = tf.exp(log_var / 2.0)\n",
    "\n",
    "        sampled_t = tf.random.normal(shape=std_dev.shape)\n",
    "        sampled_z = mean + std_dev*sampled_t\n",
    "\n",
    "        if unbatched:\n",
    "            return sampled_z[0]\n",
    "        else:\n",
    "            return sampled_z\n",
    "\n",
    "    @tf.function\n",
    "    def train_vae(self, sample):\n",
    "        ''' Sampling-based approach that gets means and standard deviations\n",
    "            from inference(sample), then per batch item uses a single std normal\n",
    "            sample to pick a vector for the generator to use.\n",
    "            (With small batches and no momentum, this can result in unexpected\n",
    "            gradients for the std deviation term. Fortunately, we are using Adam here.) '''\n",
    "        if sample.dtype == tf.uint8:\n",
    "            sample = tf.cast(sample, 'float32')/255\n",
    "        latent_info = self.inference_net(sample)\n",
    "        mean, log_var = tf.split(latent_info, [self.latent_dim, self.latent_dim], -1)\n",
    "\n",
    "        std_dev = tf.exp(log_var / 2.0)\n",
    "\n",
    "        sampled_t = tf.random.normal(shape=std_dev.shape)\n",
    "        sampled_z = mean + std_dev*sampled_t\n",
    "\n",
    "        new = self.generator_net(sampled_z)\n",
    "\n",
    "        # lifted from David Ha's github\n",
    "        # Ha replaces the traditional probability-distribution loss with two things:\n",
    "        # the error in reconstructing the sample\n",
    "        reconstruction_loss = tf.reduce_sum((sample-new)**2) / sample.shape[0]\n",
    "        # and the KL-divergence of the z-distribution from the standard normal distribution\n",
    "        kl_loss = - 0.5 * tf.reduce_sum(1 + log_var - mean**2 - tf.exp(log_var)) / sample.shape[0]\n",
    "\n",
    "        loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=new, labels=sample) # is this what we want?\n",
    "        logpsample_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        # the unconditioned latent distribution is assumed to be standard normal:\n",
    "        logpz = tf.reduce_sum(log_normal_pdf(sampled_z, 0., 1.), axis=[1])\n",
    "        logqz_sample = tf.reduce_sum(log_normal_pdf(sampled_z, mean, std_dev), axis=[1])\n",
    "        loss = -tf.reduce_mean(logpsample_z + logpz - logqz_sample)\n",
    "\n",
    "        vars = self.inference_net.trainable_variables + self.generator_net.trainable_variables\n",
    "        grads = tf.gradients(loss, vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, vars))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50445728",
   "metadata": {},
   "source": [
    "The VAE includes an encoder network and a decoder network. The encoder network consists of four convolution layers, each of which applies 2x2 filters with a stride of 2x2 followed by a ReLU nonlinearity. There are no pooling layers. The convolution layers have 32, 64, 128, and 256 filters, respectively. Following the convolution layers, the feature maps are flattened and fed through a fully-connected layer with a 16-unit linear output that parameterizes the normal distribution of latent-space embeddings of the image. (The first 8 units are the means and the second 8 are the log-variances for the distribution.) Keep in mind that, since a VAE is a probabilistic model for what sort of latent-space representation *might* have yielded the observed surface sample, there is randomness involved in the `sample_latent` method.\n",
    "\n",
    "The decoder network consists of a single fully-connected ReLU layer that maps the 8-unit latent representation onto a 1x1x1024 representation, followed by four convolution transpose layers, which consist of 128 5x5 filters applied with a stride of 2, 64 5x5 filters applied with a stride of 2, 32 7x7 filters applied with a stride of 2, and 3 6x6 filters applied with a stride of 3x3. This full architecture is identical to the one used in Schmidhuber and Ha's \"World Models\" paper from a few years ago, except that I use a much smaller 8-unit latent representation instead of their 32-unit representation.\n",
    "\n",
    "The code to generate a VAE with this specification and to train it on the half-cheetah data follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulled from Schmidhuber & Ha \"World Models\"\n",
    "def make_default_vae(image_shape=(96, 96, 3), latent_dim=8):\n",
    "    sample_inference_conv_layers = [\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=2, strides=(2, 2), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=2, strides=(2, 2), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=2, strides=(2, 2), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=2, strides=(2, 2), activation='relu')]\n",
    "\n",
    "    sample_generator_deconv_layers = [tf.keras.layers.Dense(units=1*1*1024, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Reshape(target_shape=(1, 1, 1024)),\n",
    "        tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding=\"VALID\", activation='relu'),\n",
    "        tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=5, strides=(2, 2), padding=\"VALID\", activation='relu'),\n",
    "        tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=7, strides=(2, 2), padding=\"VALID\", activation='relu'),\n",
    "        tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=6, strides=(3, 3), padding=\"VALID\")]\n",
    "\n",
    "    return VAE(sample_inference_conv_layers, sample_generator_deconv_layers, image_shape, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0086f5",
   "metadata": {},
   "source": [
    "## 5.2 Training\n",
    "\n",
    "### 5.2.1 VAE training regime\n",
    "\n",
    "The goal here is to use random rollouts to obtain a wide variety of images for the half-cheetah task, which should ideally represent a good portion of the distribution that will be encountered during reinforcement learning training. For a previous course project, I studied a possible approach to iteratively train a VAE and the RL agent it acts as a vision module for, but I do not employ that here.\n",
    "\n",
    "Code to gather and save these images follows. Note that the resulting dataset is 8 GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, rng, actor_count, action_shape, stoch_persistence):\n",
    "        self.rng = rng\n",
    "\n",
    "        self.actor_count = actor_count\n",
    "\n",
    "        self.stoch_persistence = stoch_persistence\n",
    "\n",
    "        self.action_shape = action_shape\n",
    "\n",
    "        self.cur_action = self.rng.normal(size=(self.actor_count,) + self.action_shape)\n",
    "\n",
    "    def act(self, obs, actor=0, temp=1):\n",
    "        if self.rng.random() > self.stoch_persistence:\n",
    "            self.cur_action[actor] = self.rng.normal(size=self.action_shape)\n",
    "\n",
    "        return self.cur_action[actor]\n",
    "\n",
    "    def store(self, actor, obs, action, reward, terminal):\n",
    "        pass\n",
    "\n",
    "    def train(self, epochs=1):\n",
    "        return np.array(0.), np.array(0.)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "class HalfCheetahVAETask:\n",
    "    def __init__(self, rng, gen_images=False):\n",
    "        import vae\n",
    "        self.vae = vae.make_default_vae(latent_dim=8)\n",
    "\n",
    "        self.rng = rng\n",
    "\n",
    "        self.env = gym.make(\"HalfCheetahMuJoCoEnv-v0\")\n",
    "        self.env.seed(int(rng.integers(2**63-1)))\n",
    "        self.env.env._render_width = 96\n",
    "        self.env.env._render_height = 96\n",
    "\n",
    "        self.obs_shape = (self.vae.latent_dim,)\n",
    "        self.action_shape = (6,)\n",
    "        self.cumulative_r = 0\n",
    "\n",
    "        self.gen_images = gen_images\n",
    "\n",
    "        if self.gen_images:\n",
    "            self.im_buffer = np.zeros((1024, 96, 96, 3))\n",
    "            self.im_buffer_i = 0\n",
    "        else:\n",
    "            self.vae.inference_net.load_weights('out/cheetah_inf.tfdat')\n",
    "            self.vae.generator_net.load_weights('out/cheetah_gen.tfdat')\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_r = 0\n",
    "        self.env.reset()\n",
    "\n",
    "        if self.gen_images:\n",
    "            obs = np.zeros(self.obs_shape)\n",
    "        else:\n",
    "            vis_obs = self.env.render('rgb_array')\n",
    "            obs = self.vae.sample_latent(vis_obs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        _, r, t, _ = self.env.step(action)\n",
    "\n",
    "        if (not self.gen_images) or self.rng.random() < 0.1:\n",
    "            vis_obs = self.env.render('rgb_array')\n",
    "\n",
    "            if self.gen_images:\n",
    "                self.im_buffer[self.im_buffer_i] = vis_obs\n",
    "                self.im_buffer_i += 1\n",
    "                if self.im_buffer_i == self.im_buffer.shape[0]:\n",
    "                    self.im_buffer = np.concatenate([self.im_buffer, np.zeros((1024, 96, 96, 3))], axis=0)\n",
    "\n",
    "        if self.gen_images:\n",
    "            obs = np.zeros(self.obs_shape)\n",
    "        else:\n",
    "            obs = self.vae.sample_latent(vis_obs)\n",
    "\n",
    "        self.cumulative_r += r\n",
    "        return obs, r, t, _\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "   \n",
    "    def get_return(self):\n",
    "        return self.cumulative_r\n",
    "\n",
    "    def get_samples(self):\n",
    "        return self.im_buffer[:self.im_buffer_i]\n",
    "\n",
    "def gen_random_cheetah_rollouts(seed):\n",
    "    agent_rng = np.random.default_rng(seed)\n",
    "    task_rng = np.random.default_rng(seed+234579672983459873)\n",
    "\n",
    "    task = tasks.HalfCheetahVAETask(task_rng, no_state=True)\n",
    "\n",
    "    # expected time to switch action distribution is 10 timesteps\n",
    "    ag = agent.RandomAgent(agent_rng, 1, task.action_shape, 0.931)\n",
    "\n",
    "    sim = simulation.Simulation(ag, task, 100)\n",
    "    sim.run(True)\n",
    "\n",
    "    np.save('out/half_cheetah_images.npy', task.get_samples())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c812c94",
   "metadata": {},
   "source": [
    "Following this, we will train the VAE on this dataset for 8 epochs, When training and especially when fine-tuning VAEs, I like to understand how representations are changing over time. Thus, my training code keeps track of and plots the cosine similarity of a sample from the training set as well as the overall loss terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08daf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(vae, training_dataset, distance_sampler):\n",
    "    losses = []\n",
    "    distances = []\n",
    "    first_mean = vae.get_mean(distance_sampler)\n",
    "    for epoch in range(8):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        for sample in training_dataset:\n",
    "            loss = vae.train_vae(sample)\n",
    "            losses.append(loss.numpy())\n",
    "            current_mean = vae.get_mean(distance_sampler)\n",
    "            cos = -tf.reduce_mean(tf.keras.losses.cosine_similarity(first_mean, current_mean))\n",
    "            distances.append(cos.numpy())\n",
    "            total += loss\n",
    "            count += 1\n",
    "            if count % 50 == 0:\n",
    "                print(count, loss)\n",
    "        print('Epoch', epoch, total/count)\n",
    "    plt.plot(losses, label='VAE Loss')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('VAE Loss')\n",
    "    plt.show()\n",
    "    plt.plot(distances, label='Sample Embedding Similarity')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Sample Embedding Similarity')\n",
    "    plt.show()\n",
    "\n",
    "def train_cheetah_VAE():\n",
    "    states = np.load('out/half_cheetah_images.npy')\n",
    "    np.random.seed(15)\n",
    "    np.random.shuffle(states)\n",
    "    states = np.uint8(states)\n",
    "\n",
    "    minibatch = 256\n",
    "\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices(states).batch(minibatch)\n",
    "\n",
    "    vae = make_default_vae(latent_dim=8)\n",
    "    train_vae(vae, training_dataset, states[:10])\n",
    "\n",
    "    vae.inference_net.save_weights('out/cheetah_inf.tfdat')\n",
    "    vae.generator_net.save_weights('out/cheetah_gen.tfdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9dcea",
   "metadata": {},
   "source": [
    "### 5.2.2 RL training regimes\n",
    "\n",
    "In both cases, I use the VAE as a vision module and treat it effectively as a preprocessor, expecting the RL algorithms to operate exactly they as they did with the half-cheetah's proprioceptive inputs, except for the fact that the VAE's latent space has 8 dimensions and the original observation space has 17. The code to perform this is included above in the defintion of `HalfCheetahVAETask`.\n",
    "\n",
    "#### 5.2.2.1 Training a policy for the half-cheetah VAE task using REINFORCE\n",
    "\n",
    "My regime almost precisely follows the one described in section 2.2.2. In order to train my half-cheetah policy using REINFORCE with a VAE input, I used a temporal discount factor of 0.99 (reward half-life: 62.5 timesteps, constant reward scaling factor: 100) and an action persistence of 0.931 (expected switch time: 10 timesteps). I trained for 2500 episodes (capped at 500 timesteps each) and (as mentioned before) the network architectures were [8, 160, 80, 1] (value) and [8, 160, 180, 18] (policy) with a training rate of 0.00001. The full specification for the REINFORCE half-cheetah VAE simulation is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_REINFORCE_cheetah_VAE(seed):\n",
    "    agent_rng = np.random.default_rng(seed)\n",
    "    task_rng = np.random.default_rng(seed+234579672983459873)\n",
    "\n",
    "    task = tasks.HalfCheetahVAETask(task_rng)\n",
    "\n",
    "    path = f'out/REINFORCE-cheetahVAE-{seed}.pickle'\n",
    "\n",
    "    # expected time to switch action distribution is 10 timesteps\n",
    "    policy_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, True, task.action_shape, 3)\n",
    "    value_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, False, task.action_shape, 3)\n",
    "    ag = agent.AdvantageAgent(agent_rng, 1, policy_network, value_network, 0, 0.99, 0.931, 0)\n",
    "\n",
    "    sim = simulation.Simulation(ag, task, 2500, path)\n",
    "    sim.run(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b859298",
   "metadata": {},
   "source": [
    "#### 5.2.2.2 Training a policy for the half-cheetah VAE task using A2C\n",
    "\n",
    "My regime almost precisely follows the one described in section 2.3.2. In order to train my half-cheetah policy using A2C with a VAE input, I used a temporal discount factor of 0.99 (reward half-life: 62.5 timesteps, constant reward scaling factor: 100) and an action persistence of 0.931 (expected switch time: 10 timesteps). I included an entropy term weighted by 0.05 in my objective function. I used 10 actors and trained for 250 episode cycles (2500 episodes total). As mentioned before, the network architectures were \\[8, 160, 80, 1\\] (value) and \\[8, 160, 180, 18\\] (policy) with a training rate of 0.00001. The full specification for the A2C half-cheetah VAE simulation is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11baeba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_A2C_cheetah_VAE(seed):\n",
    "    agent_rng = np.random.default_rng(seed)\n",
    "    task_rng = np.random.default_rng(seed+234579672983459873)\n",
    "\n",
    "    task = tasks.HalfCheetahVAETask(task_rng)\n",
    "\n",
    "    path = f'out/A2C-cheetahVAE-{seed}.pickle'\n",
    "\n",
    "    # expected time to switch action distribution is 20 timesteps\n",
    "    policy_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, True, task.action_shape, 3)\n",
    "    value_network = network.Network(task.obs_shape, network.FFANN_factory([160, 80]), 0.00001, False, task.action_shape, 3)\n",
    "    ag = agent.AdvantageAgent(agent_rng, 10, policy_network, value_network, 0, 0.99, 0.931, 0.05)\n",
    "\n",
    "    sim = simulation.Simulation(ag, task, 250, path)\n",
    "    sim.run(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ec7a3",
   "metadata": {},
   "source": [
    "### 5.2.3 Main VAE simulations\n",
    "\n",
    "Enable the following cell to generate new random rollouts for the half-cheetah task and train the VAE. Again, this will consume 8 GB of disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5aedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_vae_training = False\n",
    "\n",
    "# enable_vae_training = True\n",
    "\n",
    "if enable_vae_training:\n",
    "    gen_random_cheetah_rollouts(0)\n",
    "    \n",
    "    train_cheetah_VAE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0daf3e2",
   "metadata": {},
   "source": [
    "Enable and execute the following cell to repeat the experiments applying the two RL algorithms using a VAE to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc02200",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_simulations_vae = False\n",
    "\n",
    "# enable_simulations_vae = True\n",
    "\n",
    "if enable_simulations_vae:\n",
    "    for i in range(5):\n",
    "        test_REINFORCE_cheetah_VAE(i)\n",
    "        test_A2C_cheetah_VAE(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
